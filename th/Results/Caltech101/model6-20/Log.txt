2016-12-10 05:48:54 [program started on Sat Dec 10 05:48:54 2016] 
2016-12-10 05:48:54 [command line arguments] 
2016-12-10 05:48:54 stcWeights false 
2016-12-10 05:48:54 LR 0.015625 
2016-12-10 05:48:54 batchSize 100 
2016-12-10 05:48:54 network ./Models/Cifar10_Custom 
2016-12-10 05:48:54 stcNeurons true 
2016-12-10 05:48:54 constBatchSize false 
2016-12-10 05:48:54 chartFileName chart1 
2016-12-10 05:48:54 dp_prepro false 
2016-12-10 05:48:54 nGPU 1 
2016-12-10 05:48:54 dataset Caltech101 
2016-12-10 05:48:54 type cuda 
2016-12-10 05:48:54 momentum 0 
2016-12-10 05:48:54 threads 8 
2016-12-10 05:48:54 weightDecay 0 
2016-12-10 05:48:54 runningVal false 
2016-12-10 05:48:54 convLayerN 6 
2016-12-10 05:48:54 LRDecay 0 
2016-12-10 05:48:54 numHid 1024 
2016-12-10 05:48:54 save /dev/shm/clone/temp/th/Results/Caltech101/model6-20 
2016-12-10 05:48:54 augment false 
2016-12-10 05:48:54 epoch -1 
2016-12-10 05:48:54 modelsFolder ./Models/ 
2016-12-10 05:48:54 format rgb 
2016-12-10 05:48:54 preProcDir /dev/shm/clone/temp/th/PreProcData/Caltech101 
2016-12-10 05:48:54 imageFileExtension svg 
2016-12-10 05:48:54 channel 2 
2016-12-10 05:48:54 devid 9 
2016-12-10 05:48:54 visualize 1 
2016-12-10 05:48:54 LRDecayPerEpoch 0.0001 
2016-12-10 05:48:54 optimization adam 
2016-12-10 05:48:54 SBN true 
2016-12-10 05:48:54 normalization simple 
2016-12-10 05:48:54 title model1 
2016-12-10 05:48:54 load  
2016-12-10 05:48:54 whiten true 
2016-12-10 05:48:54 [----------------------] 
2016-12-10 05:48:57 ==> Network 
2016-12-10 05:48:57 nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> (10) -> (11) -> (12) -> (13) -> (14) -> (15) -> (16) -> (17) -> (18) -> (19) -> (20) -> (21) -> (22) -> (23) -> (24) -> (25) -> (26) -> (27) -> (28) -> (29) -> (30) -> (31) -> (32) -> (33) -> (34) -> (35) -> (36) -> (37) -> (38) -> output]
  (1): cudnnBinarySpatialConvolution(3 -> 256, 3x3, 1,1, 1,1)
  (2): SpatialBatchNormalizationShiftPow2
  (3): nn.HardTanh
  (4): BinarizedNeurons
  (5): cudnnBinarySpatialConvolution(256 -> 256, 3x3, 1,1, 1,1)
  (6): cudnn.SpatialMaxPooling(2x2, 2,2)
  (7): SpatialBatchNormalizationShiftPow2
  (8): nn.HardTanh
  (9): BinarizedNeurons
  (10): cudnnBinarySpatialConvolution(256 -> 512, 3x3, 1,1, 1,1)
  (11): SpatialBatchNormalizationShiftPow2
  (12): nn.HardTanh
  (13): BinarizedNeurons
  (14): cudnnBinarySpatialConvolution(512 -> 512, 3x3, 1,1, 1,1)
  (15): cudnn.SpatialMaxPooling(2x2, 2,2)
  (16): SpatialBatchNormalizationShiftPow2
  (17): nn.HardTanh
  (18): BinarizedNeurons
  (19): cudnnBinarySpatialConvolution(512 -> 1024, 3x3, 1,1, 1,1)
  (20): SpatialBatchNormalizationShiftPow2
  (21): nn.HardTanh
  (22): BinarizedNeurons
  (23): cudnnBinarySpatialConvolution(1024 -> 1024, 3x3, 1,1, 1,1)
  (24): cudnn.SpatialMaxPooling(2x2, 2,2)
  (25): SpatialBatchNormalizationShiftPow2
  (26): nn.HardTanh
  (27): BinarizedNeurons
  (28): nn.View(16384)
  (29): BinaryLinear(16384 -> 1024)
  (30): BatchNormalizationShiftPow2
  (31): nn.HardTanh
  (32): BinarizedNeurons
  (33): BinaryLinear(1024 -> 1024)
  (34): BatchNormalizationShiftPow2
  (35): nn.HardTanh
  (36): BinarizedNeurons
  (37): BinaryLinear(1024 -> 102)
  (38): nn.BatchNormalization
} 
2016-12-10 05:48:57 ==>36238898 Parameters 
2016-12-10 05:48:57 ==> Loss 
2016-12-10 05:48:57 SqrtHingeEmbeddingCriterion 
2016-12-10 05:48:57 
==> Starting Training
 
2016-12-10 05:48:57 Epoch 1 
2016-12-10 05:51:53 Training Error = 0.85176184690158 
2016-12-10 05:51:53 Training Loss = 0.41678155225225 
2016-12-10 05:51:58 Valid Error = 0.78857837181045 
2016-12-10 05:51:58 Valid Loss = 0.066351739396152 
2016-12-10 05:52:03 Test Error = 0.81222707423581 
2016-12-10 05:52:03 Test Loss = 0.06710181834651 
2016-12-10 05:52:03 -------------------LR------------------- 
2016-12-10 05:52:03 0.015625 
2016-12-10 05:52:03 Epoch 2 
2016-12-10 05:54:57 Training Error = 0.6576211691643 
2016-12-10 05:54:57 Training Loss = 0.041400096227252 
2016-12-10 05:55:02 Valid Error = 0.65370595382746 
2016-12-10 05:55:02 Valid Loss = 0.031305983668872 
2016-12-10 05:55:07 Test Error = 0.66703056768559 
2016-12-10 05:55:07 Test Loss = 0.031981168391658 
2016-12-10 05:55:07 -------------------LR------------------- 
2016-12-10 05:55:07 0.015625 
2016-12-10 05:55:07 Epoch 3 
2016-12-10 05:57:58 Training Error = 0.64169029296611 
2016-12-10 05:57:58 Training Loss = 0.030003006794962 
2016-12-10 05:58:03 Valid Error = 0.64277035236938 
2016-12-10 05:58:03 Valid Loss = 0.029079531999974 
2016-12-10 05:58:08 Test Error = 0.65829694323144 
2016-12-10 05:58:08 Test Loss = 0.030291933807672 
2016-12-10 05:58:08 -------------------LR------------------- 
2016-12-10 05:58:08 0.015625 
2016-12-10 05:58:08 Epoch 4 
2016-12-10 06:01:09 Training Error = 0.62265424598353 
2016-12-10 06:01:09 Training Loss = 0.02843572322443 
2016-12-10 06:01:14 Valid Error = 0.72053462940462 
2016-12-10 06:01:14 Valid Loss = 0.031489385605039 
2016-12-10 06:01:19 Test Error = 0.75218340611354 
2016-12-10 06:01:19 Test Loss = 0.033243965111527 
2016-12-10 06:01:19 -------------------LR------------------- 
2016-12-10 06:01:19 0.015625 
2016-12-10 06:01:19 Epoch 5 
2016-12-10 06:04:14 Training Error = 0.6045632509788 
2016-12-10 06:04:14 Training Loss = 0.027386623376157 
2016-12-10 06:04:19 Valid Error = 0.75698663426488 
2016-12-10 06:04:19 Valid Loss = 0.031190703929436 
2016-12-10 06:04:24 Test Error = 0.79148471615721 
2016-12-10 06:04:24 Test Loss = 0.033131150722504 
2016-12-10 06:04:24 -------------------LR------------------- 
2016-12-10 06:04:24 0.015625 
2016-12-10 06:04:24 Epoch 6 
2016-12-10 06:07:43 Training Error = 0.59862292426083 
2016-12-10 06:07:43 Training Loss = 0.027017089369317 
2016-12-10 06:07:47 Valid Error = 0.75212636695018 
2016-12-10 06:07:47 Valid Loss = 0.031569546526712 
2016-12-10 06:07:53 Test Error = 0.78711790393013 
2016-12-10 06:07:53 Test Loss = 0.033022454205681 
2016-12-10 06:07:53 -------------------LR------------------- 
2016-12-10 06:07:53 0.015625 
2016-12-10 06:07:53 Epoch 7 
2016-12-10 06:10:50 Training Error = 0.57270149858242 
2016-12-10 06:10:50 Training Loss = 0.02643348687009 
2016-12-10 06:10:54 Valid Error = 0.75212636695018 
2016-12-10 06:10:54 Valid Loss = 0.032658202827253 
2016-12-10 06:11:00 Test Error = 0.79257641921397 
2016-12-10 06:11:00 Test Loss = 0.034492416681028 
2016-12-10 06:11:00 -------------------LR------------------- 
2016-12-10 06:11:00 0.015625 
2016-12-10 06:11:00 Epoch 8 
2016-12-10 06:14:06 Training Error = 0.55879573376536 
2016-12-10 06:14:06 Training Loss = 0.02633024545593 
2016-12-10 06:14:11 Valid Error = 0.63304981773998 
2016-12-10 06:14:11 Valid Loss = 0.027398126328817 
2016-12-10 06:14:16 Test Error = 0.65393013100437 
2016-12-10 06:14:16 Test Loss = 0.029348771067227 
2016-12-10 06:14:16 -------------------LR------------------- 
2016-12-10 06:14:16 0.015625 
2016-12-10 06:14:16 Epoch 9 
2016-12-10 06:17:15 Training Error = 0.53989469420818 
2016-12-10 06:17:15 Training Loss = 0.025366207388514 
2016-12-10 06:17:20 Valid Error = 0.76549210206561 
2016-12-10 06:17:20 Valid Loss = 0.030762870725701 
2016-12-10 06:17:25 Test Error = 0.78930131004367 
2016-12-10 06:17:25 Test Loss = 0.032407367996141 
2016-12-10 06:17:25 -------------------LR------------------- 
2016-12-10 06:17:25 0.015625 
2016-12-10 06:17:25 Epoch 10 
2016-12-10 06:20:30 Training Error = 0.52328878088295 
2016-12-10 06:20:30 Training Loss = 0.025216583961465 
2016-12-10 06:20:35 Valid Error = 0.62940461725395 
2016-12-10 06:20:35 Valid Loss = 0.028655032459348 
2016-12-10 06:20:40 Test Error = 0.69323144104803 
2016-12-10 06:20:40 Test Loss = 0.03225135790133 
2016-12-10 06:20:40 -------------------LR------------------- 
2016-12-10 06:20:40 0.015625 
2016-12-10 06:20:40 Epoch 11 
2016-12-10 06:23:34 Training Error = 0.51262319427569 
2016-12-10 06:23:34 Training Loss = 0.024839863431131 
2016-12-10 06:23:39 Valid Error = 0.54070473876063 
2016-12-10 06:23:39 Valid Loss = 0.027960706817231 
2016-12-10 06:23:44 Test Error = 0.58624454148472 
2016-12-10 06:23:44 Test Loss = 0.0318394093046 
2016-12-10 06:23:44 -------------------LR------------------- 
2016-12-10 06:23:44 0.015625 
2016-12-10 06:23:44 Epoch 12 
2016-12-10 06:26:44 Training Error = 0.51815849871743 
2016-12-10 06:26:44 Training Loss = 0.025018939563741 
2016-12-10 06:26:49 Valid Error = 0.54070473876063 
2016-12-10 06:26:49 Valid Loss = 0.028419643473284 
2016-12-10 06:26:55 Test Error = 0.58624454148472 
2016-12-10 06:26:55 Test Loss = 0.032320691090004 
2016-12-10 06:26:55 -------------------LR------------------- 
2016-12-10 06:26:55 0.015625 
2016-12-10 06:26:55 Epoch 13 
2016-12-10 06:30:01 Training Error = 0.5161333873363 
2016-12-10 06:30:01 Training Loss = 0.02482661565786 
2016-12-10 06:30:06 Valid Error = 0.53462940461725 
2016-12-10 06:30:06 Valid Loss = 0.02692799770438 
2016-12-10 06:30:11 Test Error = 0.58187772925764 
2016-12-10 06:30:11 Test Loss = 0.030875250311459 
2016-12-10 06:30:11 -------------------LR------------------- 
2016-12-10 06:30:11 0.015625 
2016-12-10 06:30:11 Epoch 14 
2016-12-10 06:33:12 Training Error = 0.51667341703794 
2016-12-10 06:33:12 Training Loss = 0.02473149223577 
2016-12-10 06:33:17 Valid Error = 0.61846901579587 
2016-12-10 06:33:17 Valid Loss = 0.029242709391924 
2016-12-10 06:33:22 Test Error = 0.68013100436681 
2016-12-10 06:33:22 Test Loss = 0.033461987738516 
2016-12-10 06:33:22 -------------------LR------------------- 
2016-12-10 06:33:22 0.015625 
2016-12-10 06:33:22 Epoch 15 
2016-12-10 06:36:23 Training Error = 0.51383826110436 
2016-12-10 06:36:23 Training Loss = 0.024821977336467 
2016-12-10 06:36:28 Valid Error = 0.54070473876063 
2016-12-10 06:36:28 Valid Loss = 0.029676759030948 
2016-12-10 06:36:33 Test Error = 0.58624454148472 
2016-12-10 06:36:33 Test Loss = 0.034113179393843 
2016-12-10 06:36:33 -------------------LR------------------- 
2016-12-10 06:36:33 0.015625 
2016-12-10 06:36:33 Epoch 16 
2016-12-10 06:39:35 Training Error = 0.5176184690158 
2016-12-10 06:39:35 Training Loss = 0.024969122835902 
2016-12-10 06:39:40 Valid Error = 0.54070473876063 
2016-12-10 06:39:40 Valid Loss = 0.029388266293583 
2016-12-10 06:39:45 Test Error = 0.58624454148472 
2016-12-10 06:39:45 Test Loss = 0.033552919556113 
2016-12-10 06:39:45 -------------------LR------------------- 
2016-12-10 06:39:45 0.015625 
2016-12-10 06:39:45 Epoch 17 
2016-12-10 06:42:45 Training Error = 0.51505332793304 
2016-12-10 06:42:45 Training Loss = 0.02481570291813 
2016-12-10 06:42:50 Valid Error = 0.54434993924666 
2016-12-10 06:42:50 Valid Loss = 0.028134881139106 
2016-12-10 06:42:55 Test Error = 0.59170305676856 
2016-12-10 06:42:55 Test Loss = 0.032055079329248 
2016-12-10 06:42:55 -------------------LR------------------- 
2016-12-10 06:42:55 0.015625 
2016-12-10 06:42:55 Epoch 18 
2016-12-10 06:45:58 Training Error = 0.51194815714864 
2016-12-10 06:45:58 Training Loss = 0.024876526307748 
2016-12-10 06:46:03 Valid Error = 0.54434993924666 
2016-12-10 06:46:03 Valid Loss = 0.02921734113116 
2016-12-10 06:46:08 Test Error = 0.59170305676856 
2016-12-10 06:46:08 Test Loss = 0.033483777307997 
2016-12-10 06:46:08 -------------------LR------------------- 
2016-12-10 06:46:08 0.015625 
2016-12-10 06:46:08 Epoch 19 
2016-12-10 06:49:16 Training Error = 0.51694343188875 
2016-12-10 06:49:16 Training Loss = 0.024771387976781 
2016-12-10 06:49:20 Valid Error = 0.54678007290401 
2016-12-10 06:49:20 Valid Loss = 0.02767225671833 
2016-12-10 06:49:26 Test Error = 0.58951965065502 
2016-12-10 06:49:26 Test Loss = 0.031714524699192 
2016-12-10 06:49:26 -------------------LR------------------- 
2016-12-10 06:49:26 0.015625 
2016-12-10 06:49:26 Epoch 20 
