2016-12-10 06:15:50 [program started on Sat Dec 10 06:15:50 2016] 
2016-12-10 06:15:50 [command line arguments] 
2016-12-10 06:15:50 stcWeights false 
2016-12-10 06:15:50 LR 0.015625 
2016-12-10 06:15:50 batchSize 500 
2016-12-10 06:15:50 network ./Models/Cifar10_Custom 
2016-12-10 06:15:50 stcNeurons true 
2016-12-10 06:15:50 constBatchSize false 
2016-12-10 06:15:50 chartFileName chart1 
2016-12-10 06:15:50 dp_prepro false 
2016-12-10 06:15:50 nGPU 3 
2016-12-10 06:15:50 dataset Cifar100 
2016-12-10 06:15:50 type cuda 
2016-12-10 06:15:50 momentum 0 
2016-12-10 06:15:50 threads 8 
2016-12-10 06:15:50 weightDecay 0 
2016-12-10 06:15:50 runningVal false 
2016-12-10 06:15:50 convLayerN 6 
2016-12-10 06:15:50 LRDecay 0 
2016-12-10 06:15:50 numHid 1024 
2016-12-10 06:15:50 save /dev/shm/clone/temp/th/Results/Cifar100/model6-14 
2016-12-10 06:15:50 augment false 
2016-12-10 06:15:50 epoch -1 
2016-12-10 06:15:50 modelsFolder ./Models/ 
2016-12-10 06:15:50 format rgb 
2016-12-10 06:15:50 preProcDir /dev/shm/clone/temp/th/PreProcData/Cifar100 
2016-12-10 06:15:50 imageFileExtension svg 
2016-12-10 06:15:50 channel 1.4 
2016-12-10 06:15:50 devid 6 
2016-12-10 06:15:50 visualize 1 
2016-12-10 06:15:50 LRDecayPerEpoch 0.0001 
2016-12-10 06:15:50 optimization adam 
2016-12-10 06:15:50 SBN true 
2016-12-10 06:15:50 normalization simple 
2016-12-10 06:15:50 title model1 
2016-12-10 06:15:50 load  
2016-12-10 06:15:50 whiten true 
2016-12-10 06:15:50 [----------------------] 
2016-12-10 06:15:58 ==> Network 
2016-12-10 06:15:58 DataParallelTable: 3 x nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> (10) -> (11) -> (12) -> (13) -> (14) -> (15) -> (16) -> (17) -> (18) -> (19) -> (20) -> (21) -> (22) -> (23) -> (24) -> (25) -> (26) -> (27) -> (28) -> (29) -> (30) -> (31) -> (32) -> (33) -> (34) -> (35) -> (36) -> (37) -> (38) -> output]
  (1): cudnnBinarySpatialConvolution(3 -> 179, 3x3, 1,1, 1,1)
  (2): SpatialBatchNormalizationShiftPow2
  (3): nn.HardTanh
  (4): BinarizedNeurons
  (5): cudnnBinarySpatialConvolution(179 -> 179, 3x3, 1,1, 1,1)
  (6): cudnn.SpatialMaxPooling(2x2, 2,2)
  (7): SpatialBatchNormalizationShiftPow2
  (8): nn.HardTanh
  (9): BinarizedNeurons
  (10): cudnnBinarySpatialConvolution(179 -> 358, 3x3, 1,1, 1,1)
  (11): SpatialBatchNormalizationShiftPow2
  (12): nn.HardTanh
  (13): BinarizedNeurons
  (14): cudnnBinarySpatialConvolution(358 -> 358, 3x3, 1,1, 1,1)
  (15): cudnn.SpatialMaxPooling(2x2, 2,2)
  (16): SpatialBatchNormalizationShiftPow2
  (17): nn.HardTanh
  (18): BinarizedNeurons
  (19): cudnnBinarySpatialConvolution(358 -> 716, 3x3, 1,1, 1,1)
  (20): SpatialBatchNormalizationShiftPow2
  (21): nn.HardTanh
  (22): BinarizedNeurons
  (23): cudnnBinarySpatialConvolution(716 -> 716, 3x3, 1,1, 1,1)
  (24): cudnn.SpatialMaxPooling(2x2, 2,2)
  (25): SpatialBatchNormalizationShiftPow2
  (26): nn.HardTanh
  (27): BinarizedNeurons
  (28): nn.View(11456)
  (29): BinaryLinear(11456 -> 1024)
  (30): BatchNormalizationShiftPow2
  (31): nn.HardTanh
  (32): BinarizedNeurons
  (33): BinaryLinear(1024 -> 1024)
  (34): BatchNormalizationShiftPow2
  (35): nn.HardTanh
  (36): BinarizedNeurons
  (37): BinaryLinear(1024 -> 100)
  (38): nn.BatchNormalization
} 
2016-12-10 06:15:58 ==>21840154 Parameters 
2016-12-10 06:15:58 ==> Loss 
2016-12-10 06:15:58 SqrtHingeEmbeddingCriterion 
2016-12-10 06:15:58 
==> Starting Training
 
2016-12-10 06:15:58 Epoch 1 
2016-12-10 06:23:59 Training Error = 0.94357777777778 
2016-12-10 06:23:59 Training Loss = 0.3496757023112 
2016-12-10 06:24:05 Valid Error = 0.92838567713543 
2016-12-10 06:24:05 Valid Loss = 0.057760106134143 
2016-12-10 06:24:18 Test Error = 0.9268 
2016-12-10 06:24:18 Test Loss = 0.057542012451172 
2016-12-10 06:24:18 -------------------LR------------------- 
2016-12-10 06:24:18 0.015625 
2016-12-10 06:24:18 Epoch 2 
2016-12-10 06:32:59 Training Error = 0.91768888888889 
2016-12-10 06:32:59 Training Loss = 0.042488517686632 
2016-12-10 06:33:05 Valid Error = 0.91078215643129 
2016-12-10 06:33:05 Valid Loss = 0.040773384830697 
2016-12-10 06:33:18 Test Error = 0.9125 
2016-12-10 06:33:18 Test Loss = 0.040768573730469 
2016-12-10 06:33:18 -------------------LR------------------- 
2016-12-10 06:33:18 0.015625 
2016-12-10 06:33:18 Epoch 3 
2016-12-10 06:41:56 Training Error = 0.89151111111111 
2016-12-10 06:41:56 Training Loss = 0.038772018581814 
2016-12-10 06:42:03 Valid Error = 0.89077815563113 
2016-12-10 06:42:03 Valid Loss = 0.039627405236743 
2016-12-10 06:42:15 Test Error = 0.8915 
2016-12-10 06:42:15 Test Loss = 0.039715894897461 
2016-12-10 06:42:15 -------------------LR------------------- 
2016-12-10 06:42:15 0.015625 
2016-12-10 06:42:15 Epoch 4 
2016-12-10 06:51:06 Training Error = 0.85802222222222 
2016-12-10 06:51:06 Training Loss = 0.038128007378472 
2016-12-10 06:51:12 Valid Error = 0.87237447489498 
2016-12-10 06:51:12 Valid Loss = 0.039111539098705 
2016-12-10 06:51:25 Test Error = 0.8714 
2016-12-10 06:51:25 Test Loss = 0.039231463623047 
2016-12-10 06:51:25 -------------------LR------------------- 
2016-12-10 06:51:25 0.015625 
2016-12-10 06:51:25 Epoch 5 
2016-12-10 07:01:31 Training Error = 0.83008888888889 
2016-12-10 07:01:31 Training Loss = 0.037591878282335 
2016-12-10 07:01:37 Valid Error = 0.8375675135027 
2016-12-10 07:01:37 Valid Loss = 0.03814125702626 
2016-12-10 07:01:50 Test Error = 0.8393 
2016-12-10 07:01:50 Test Loss = 0.038269890258789 
2016-12-10 07:01:50 -------------------LR------------------- 
2016-12-10 07:01:50 0.015625 
2016-12-10 07:01:50 Epoch 6 
2016-12-10 07:12:03 Training Error = 0.79353333333333 
2016-12-10 07:12:03 Training Loss = 0.037056679307726 
2016-12-10 07:12:10 Valid Error = 0.80876175235047 
2016-12-10 07:12:10 Valid Loss = 0.037335204937218 
2016-12-10 07:12:22 Test Error = 0.8023 
2016-12-10 07:12:22 Test Loss = 0.037413943969727 
2016-12-10 07:12:22 -------------------LR------------------- 
2016-12-10 07:12:22 0.015625 
2016-12-10 07:12:22 Epoch 7 
2016-12-10 07:22:43 Training Error = 0.76717777777778 
2016-12-10 07:22:43 Training Loss = 0.036540249620226 
2016-12-10 07:22:50 Valid Error = 0.79895979195839 
2016-12-10 07:22:50 Valid Loss = 0.03711385603727 
2016-12-10 07:23:02 Test Error = 0.7945 
2016-12-10 07:23:02 Test Loss = 0.037192432373047 
2016-12-10 07:23:02 -------------------LR------------------- 
2016-12-10 07:23:02 0.015625 
2016-12-10 07:23:03 Epoch 8 
2016-12-10 07:33:18 Training Error = 0.741 
2016-12-10 07:33:18 Training Loss = 0.036084649522569 
2016-12-10 07:33:25 Valid Error = 0.74654930986197 
2016-12-10 07:33:25 Valid Loss = 0.036097177144328 
2016-12-10 07:33:38 Test Error = 0.7454 
2016-12-10 07:33:38 Test Loss = 0.036232748413086 
2016-12-10 07:33:38 -------------------LR------------------- 
2016-12-10 07:33:38 0.015625 
2016-12-10 07:33:38 Epoch 9 
2016-12-10 07:43:49 Training Error = 0.71802222222222 
2016-12-10 07:43:49 Training Loss = 0.035659666205512 
2016-12-10 07:43:55 Valid Error = 0.73394678935787 
2016-12-10 07:43:55 Valid Loss = 0.03582079965742 
2016-12-10 07:44:08 Test Error = 0.7317 
2016-12-10 07:44:08 Test Loss = 0.035882817016602 
2016-12-10 07:44:08 -------------------LR------------------- 
2016-12-10 07:44:08 0.015625 
2016-12-10 07:44:08 Epoch 10 
