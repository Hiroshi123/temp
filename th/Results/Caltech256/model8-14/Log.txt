2016-12-10 06:27:28 [program started on Sat Dec 10 06:27:28 2016] 
2016-12-10 06:27:28 [command line arguments] 
2016-12-10 06:27:28 stcWeights false 
2016-12-10 06:27:28 LR 0.015625 
2016-12-10 06:27:28 batchSize 64 
2016-12-10 06:27:28 network ./Models/Cifar10_Custom 
2016-12-10 06:27:28 stcNeurons true 
2016-12-10 06:27:28 constBatchSize false 
2016-12-10 06:27:28 chartFileName chart1 
2016-12-10 06:27:28 dp_prepro false 
2016-12-10 06:27:28 nGPU 1 
2016-12-10 06:27:28 dataset Caltech256 
2016-12-10 06:27:28 type cuda 
2016-12-10 06:27:28 momentum 0 
2016-12-10 06:27:28 threads 8 
2016-12-10 06:27:28 weightDecay 0 
2016-12-10 06:27:28 runningVal false 
2016-12-10 06:27:28 convLayerN 8 
2016-12-10 06:27:28 LRDecay 0 
2016-12-10 06:27:28 numHid 1024 
2016-12-10 06:27:28 save /dev/shm/clone/temp/th/Results/Caltech256/model8-14 
2016-12-10 06:27:28 augment false 
2016-12-10 06:27:28 epoch -1 
2016-12-10 06:27:28 modelsFolder ./Models/ 
2016-12-10 06:27:28 format rgb 
2016-12-10 06:27:28 preProcDir /dev/shm/clone/temp/th/PreProcData/Caltech256 
2016-12-10 06:27:28 imageFileExtension svg 
2016-12-10 06:27:28 channel 1.4 
2016-12-10 06:27:28 devid 3 
2016-12-10 06:27:28 visualize 1 
2016-12-10 06:27:28 LRDecayPerEpoch 0.0001 
2016-12-10 06:27:28 optimization adam 
2016-12-10 06:27:28 SBN true 
2016-12-10 06:27:28 normalization simple 
2016-12-10 06:27:28 title model1 
2016-12-10 06:27:28 load  
2016-12-10 06:27:28 whiten true 
2016-12-10 06:27:28 [----------------------] 
2016-12-10 06:27:30 ==> Network 
2016-12-10 06:27:30 nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> (10) -> (11) -> (12) -> (13) -> (14) -> (15) -> (16) -> (17) -> (18) -> (19) -> (20) -> (21) -> (22) -> (23) -> (24) -> (25) -> (26) -> (27) -> (28) -> (29) -> (30) -> (31) -> (32) -> (33) -> (34) -> (35) -> (36) -> (37) -> (38) -> (39) -> (40) -> (41) -> (42) -> (43) -> (44) -> (45) -> (46) -> (47) -> output]
  (1): cudnnBinarySpatialConvolution(3 -> 179, 3x3, 1,1, 1,1)
  (2): SpatialBatchNormalizationShiftPow2
  (3): nn.HardTanh
  (4): BinarizedNeurons
  (5): cudnnBinarySpatialConvolution(179 -> 179, 3x3, 1,1, 1,1)
  (6): cudnn.SpatialMaxPooling(2x2, 2,2)
  (7): SpatialBatchNormalizationShiftPow2
  (8): nn.HardTanh
  (9): BinarizedNeurons
  (10): cudnnBinarySpatialConvolution(179 -> 358, 3x3, 1,1, 1,1)
  (11): SpatialBatchNormalizationShiftPow2
  (12): nn.HardTanh
  (13): BinarizedNeurons
  (14): cudnnBinarySpatialConvolution(358 -> 358, 3x3, 1,1, 1,1)
  (15): cudnn.SpatialMaxPooling(2x2, 2,2)
  (16): SpatialBatchNormalizationShiftPow2
  (17): nn.HardTanh
  (18): BinarizedNeurons
  (19): cudnnBinarySpatialConvolution(358 -> 716, 3x3, 1,1, 1,1)
  (20): SpatialBatchNormalizationShiftPow2
  (21): nn.HardTanh
  (22): BinarizedNeurons
  (23): cudnnBinarySpatialConvolution(716 -> 716, 3x3, 1,1, 1,1)
  (24): cudnn.SpatialMaxPooling(2x2, 2,2)
  (25): SpatialBatchNormalizationShiftPow2
  (26): nn.HardTanh
  (27): BinarizedNeurons
  (28): cudnnBinarySpatialConvolution(716 -> 716, 3x3, 1,1, 1,1)
  (29): SpatialBatchNormalizationShiftPow2
  (30): nn.HardTanh
  (31): BinarizedNeurons
  (32): cudnnBinarySpatialConvolution(716 -> 716, 3x3, 1,1, 1,1)
  (33): cudnn.SpatialMaxPooling(2x2, 2,2)
  (34): SpatialBatchNormalizationShiftPow2
  (35): nn.HardTanh
  (36): BinarizedNeurons
  (37): nn.View(2864)
  (38): BinaryLinear(2864 -> 1024)
  (39): BatchNormalizationShiftPow2
  (40): nn.HardTanh
  (41): BinarizedNeurons
  (42): BinaryLinear(1024 -> 1024)
  (43): BatchNormalizationShiftPow2
  (44): nn.HardTanh
  (45): BinarizedNeurons
  (46): BinaryLinear(1024 -> 255)
  (47): nn.BatchNormalization
} 
2016-12-10 06:27:30 ==>22433235 Parameters 
2016-12-10 06:27:30 ==> Loss 
2016-12-10 06:27:30 SqrtHingeEmbeddingCriterion 
2016-12-10 06:27:30 
==> Starting Training
 
2016-12-10 06:27:30 Epoch 1 
2016-12-10 06:39:10 Training Error = 0.98185903303653 
2016-12-10 06:39:10 Training Loss = 0.11404084863363 
2016-12-10 06:39:21 Valid Error = 0.93448146761513 
2016-12-10 06:39:21 Valid Loss = 0.015711522983335 
2016-12-10 06:39:34 Test Error = 0.94543617379589 
2016-12-10 06:39:34 Test Loss = 0.015723930253056 
2016-12-10 06:39:34 -------------------LR------------------- 
2016-12-10 06:39:34 0.015625 
2016-12-10 06:39:34 Epoch 2 
2016-12-10 06:51:14 Training Error = 0.95327452775235 
2016-12-10 06:51:14 Training Loss = 0.01586595138113 
2016-12-10 06:51:25 Valid Error = 0.93448146761513 
2016-12-10 06:51:25 Valid Loss = 0.015860011825465 
2016-12-10 06:51:37 Test Error = 0.94543617379589 
2016-12-10 06:51:37 Test Loss = 0.015718549655187 
2016-12-10 06:51:37 -------------------LR------------------- 
2016-12-10 06:51:37 0.015625 
2016-12-10 06:51:37 Epoch 3 
2016-12-10 07:04:58 Training Error = 0.94195722726138 
2016-12-10 07:04:58 Training Loss = 0.015605521240411 
2016-12-10 07:05:09 Valid Error = 0.93448146761513 
2016-12-10 07:05:09 Valid Loss = 0.015634598183893 
2016-12-10 07:05:21 Test Error = 0.94543617379589 
2016-12-10 07:05:21 Test Loss = 0.015604071198178 
2016-12-10 07:05:21 -------------------LR------------------- 
2016-12-10 07:05:21 0.015625 
2016-12-10 07:05:21 Epoch 4 
2016-12-10 07:18:52 Training Error = 0.9412082882583 
2016-12-10 07:18:52 Training Loss = 0.015532125440731 
2016-12-10 07:19:03 Valid Error = 0.93448146761513 
2016-12-10 07:19:03 Valid Loss = 0.015564983964409 
2016-12-10 07:19:15 Test Error = 0.94543617379589 
2016-12-10 07:19:15 Test Loss = 0.015571388979081 
2016-12-10 07:19:15 -------------------LR------------------- 
2016-12-10 07:19:15 0.015625 
2016-12-10 07:19:15 Epoch 5 
2016-12-10 07:32:48 Training Error = 0.93679786968461 
2016-12-10 07:32:48 Training Loss = 0.01548753389859 
2016-12-10 07:32:59 Valid Error = 0.93934855859229 
2016-12-10 07:32:59 Valid Loss = 0.015515771108638 
2016-12-10 07:33:11 Test Error = 0.9514988211519 
2016-12-10 07:33:11 Test Loss = 0.015534826602546 
2016-12-10 07:33:11 -------------------LR------------------- 
2016-12-10 07:33:11 0.015625 
2016-12-10 07:33:11 Epoch 6 
2016-12-10 07:46:36 Training Error = 0.93151368894067 
2016-12-10 07:46:36 Training Loss = 0.015435131973932 
2016-12-10 07:46:47 Valid Error = 0.92137776113815 
2016-12-10 07:46:47 Valid Loss = 0.015529928913223 
2016-12-10 07:46:59 Test Error = 0.93331087908387 
2016-12-10 07:46:59 Test Loss = 0.015557999608116 
2016-12-10 07:46:59 -------------------LR------------------- 
2016-12-10 07:46:59 0.015625 
2016-12-10 07:46:59 Epoch 7 
2016-12-10 07:59:47 Training Error = 0.9276025630357 
2016-12-10 07:59:47 Training Loss = 0.015297994259317 
2016-12-10 07:59:58 Valid Error = 0.95394983152377 
2016-12-10 07:59:58 Valid Loss = 0.015738930810106 
2016-12-10 08:00:10 Test Error = 0.9511620074099 
2016-12-10 08:00:10 Test Loss = 0.015668609613571 
2016-12-10 08:00:10 -------------------LR------------------- 
2016-12-10 08:00:10 0.015625 
2016-12-10 08:00:10 Epoch 8 
2016-12-10 08:11:37 Training Error = 0.91628526254473 
2016-12-10 08:11:37 Training Loss = 0.015095663142499 
2016-12-10 08:11:48 Valid Error = 0.96405840509173 
2016-12-10 08:11:48 Valid Loss = 0.016030568793962 
2016-12-10 08:12:00 Test Error = 0.96295048837993 
2016-12-10 08:12:00 Test Loss = 0.016025170452355 
2016-12-10 08:12:00 -------------------LR------------------- 
2016-12-10 08:12:00 0.015625 
2016-12-10 08:12:00 Epoch 9 
2016-12-10 08:23:31 Training Error = 0.9015977365399 
2016-12-10 08:23:31 Training Loss = 0.015002357722054 
2016-12-10 08:23:42 Valid Error = 0.94346686634219 
2016-12-10 08:23:42 Valid Loss = 0.015548038036493 
2016-12-10 08:23:54 Test Error = 0.95520377231391 
2016-12-10 08:23:54 Test Loss = 0.015628588925912 
2016-12-10 08:23:54 -------------------LR------------------- 
2016-12-10 08:23:54 0.015625 
2016-12-10 08:23:54 Epoch 10 
2016-12-10 08:35:44 Training Error = 0.88749271864858 
2016-12-10 08:35:44 Training Loss = 0.014840773917224 
2016-12-10 08:35:56 Valid Error = 0.89853987270685 
2016-12-10 08:35:56 Valid Loss = 0.01511903603398 
2016-12-10 08:36:08 Test Error = 0.91108117211182 
2016-12-10 08:36:08 Test Loss = 0.015345250461914 
2016-12-10 08:36:08 -------------------LR------------------- 
2016-12-10 08:36:08 0.015625 
2016-12-10 08:36:08 Epoch 11 
2016-12-10 08:48:04 Training Error = 0.87405342431555 
2016-12-10 08:48:04 Training Loss = 0.014719105137208 
2016-12-10 08:48:15 Valid Error = 0.90565331336578 
2016-12-10 08:48:15 Valid Loss = 0.015304370995658 
2016-12-10 08:48:27 Test Error = 0.91781744695184 
2016-12-10 08:48:27 Test Loss = 0.015499220866783 
2016-12-10 08:48:27 -------------------LR------------------- 
2016-12-10 08:48:27 0.015625 
2016-12-10 08:48:27 Epoch 12 
2016-12-10 09:00:15 Training Error = 0.87455271698427 
2016-12-10 09:00:15 Training Loss = 0.01471537440308 
2016-12-10 09:00:25 Valid Error = 0.89853987270685 
2016-12-10 09:00:25 Valid Loss = 0.015353315635856 
2016-12-10 09:00:38 Test Error = 0.91108117211182 
2016-12-10 09:00:38 Test Loss = 0.01553828363335 
2016-12-10 09:00:38 -------------------LR------------------- 
2016-12-10 09:00:38 0.015625 
2016-12-10 09:00:38 Epoch 13 
2016-12-10 09:12:27 Training Error = 0.87243072314222 
2016-12-10 09:12:27 Training Loss = 0.014713481151036 
2016-12-10 09:12:38 Valid Error = 0.90565331336578 
2016-12-10 09:12:38 Valid Loss = 0.015346402043175 
2016-12-10 09:12:51 Test Error = 0.91781744695184 
2016-12-10 09:12:51 Test Loss = 0.015544912649385 
2016-12-10 09:12:51 -------------------LR------------------- 
2016-12-10 09:12:51 0.015625 
2016-12-10 09:12:51 Epoch 14 
2016-12-10 09:24:38 Training Error = 0.87513522509778 
2016-12-10 09:24:38 Training Loss = 0.014715158064797 
2016-12-10 09:24:49 Valid Error = 0.90565331336578 
2016-12-10 09:24:49 Valid Loss = 0.015430867408383 
2016-12-10 09:25:02 Test Error = 0.91781744695184 
2016-12-10 09:25:02 Test Loss = 0.015632074344143 
2016-12-10 09:25:02 -------------------LR------------------- 
2016-12-10 09:25:02 0.015625 
2016-12-10 09:25:02 Epoch 15 
2016-12-10 09:36:52 Training Error = 0.87284680036615 
2016-12-10 09:36:52 Training Loss = 0.014723775059101 
2016-12-10 09:37:03 Valid Error = 0.89853987270685 
2016-12-10 09:37:03 Valid Loss = 0.015298661878842 
2016-12-10 09:37:15 Test Error = 0.91108117211182 
2016-12-10 09:37:15 Test Loss = 0.015496168781928 
2016-12-10 09:37:15 -------------------LR------------------- 
2016-12-10 09:37:15 0.015625 
2016-12-10 09:37:15 Epoch 16 
2016-12-10 09:49:11 Training Error = 0.87434467837231 
2016-12-10 09:49:11 Training Loss = 0.014712283852443 
2016-12-10 09:49:22 Valid Error = 0.89853987270685 
2016-12-10 09:49:22 Valid Loss = 0.015255127726208 
2016-12-10 09:49:35 Test Error = 0.91108117211182 
2016-12-10 09:49:35 Test Loss = 0.015444327545007 
2016-12-10 09:49:35 -------------------LR------------------- 
2016-12-10 09:49:35 0.015625 
2016-12-10 09:49:35 Epoch 17 
2016-12-10 10:01:25 Training Error = 0.87480236331863 
2016-12-10 10:01:25 Training Loss = 0.014719993415653 
2016-12-10 10:01:36 Valid Error = 0.90490453013852 
2016-12-10 10:01:36 Valid Loss = 0.015303931075323 
2016-12-10 10:01:48 Test Error = 0.91748063320983 
2016-12-10 10:01:48 Test Loss = 0.015497554083591 
2016-12-10 10:01:48 -------------------LR------------------- 
2016-12-10 10:01:48 0.015625 
2016-12-10 10:01:48 Epoch 18 
2016-12-10 10:13:44 Training Error = 0.87413663976034 
2016-12-10 10:13:44 Training Loss = 0.014713516061053 
2016-12-10 10:13:55 Valid Error = 0.90565331336578 
2016-12-10 10:13:55 Valid Loss = 0.015395125394158 
2016-12-10 10:14:07 Test Error = 0.91781744695184 
2016-12-10 10:14:07 Test Loss = 0.015603306534393 
2016-12-10 10:14:07 -------------------LR------------------- 
2016-12-10 10:14:07 0.015625 
2016-12-10 10:14:07 Epoch 19 
2016-12-10 10:26:14 Training Error = 0.87384538570359 
2016-12-10 10:26:14 Training Loss = 0.014718100159513 
2016-12-10 10:26:25 Valid Error = 0.90565331336578 
2016-12-10 10:26:25 Valid Loss = 0.015235366892241 
2016-12-10 10:26:37 Test Error = 0.91781744695184 
2016-12-10 10:26:37 Test Loss = 0.015429630684564 
2016-12-10 10:26:37 -------------------LR------------------- 
2016-12-10 10:26:37 0.015625 
2016-12-10 10:26:37 Epoch 20 
2016-12-10 10:38:48 Training Error = 0.87197303819589 
2016-12-10 10:38:48 Training Loss = 0.014705014445304 
2016-12-10 10:38:59 Valid Error = 0.89853987270685 
2016-12-10 10:38:59 Valid Loss = 0.01528316938444 
2016-12-10 10:39:12 Test Error = 0.91108117211182 
2016-12-10 10:39:12 Test Loss = 0.015470319755882 
2016-12-10 10:39:12 -------------------LR------------------- 
2016-12-10 10:39:12 0.015625 
2016-12-10 10:39:12 Epoch 21 
2016-12-10 10:51:20 Training Error = 0.87309644670051 
2016-12-10 10:51:20 Training Loss = 0.014715887103515 
2016-12-10 10:51:32 Valid Error = 0.90565331336578 
2016-12-10 10:51:32 Valid Loss = 0.015348812242485 
2016-12-10 10:51:44 Test Error = 0.91781744695184 
2016-12-10 10:51:44 Test Loss = 0.015548608345442 
2016-12-10 10:51:44 -------------------LR------------------- 
2016-12-10 10:51:44 0.015625 
2016-12-10 10:51:44 Epoch 22 
2016-12-10 11:03:48 Training Error = 0.87342930847965 
2016-12-10 11:03:48 Training Loss = 0.014719859190451 
2016-12-10 11:03:59 Valid Error = 0.89853987270685 
2016-12-10 11:03:59 Valid Loss = 0.015210525058394 
2016-12-10 11:04:11 Test Error = 0.91108117211182 
2016-12-10 11:04:11 Test Loss = 0.015408794738871 
2016-12-10 11:04:11 -------------------LR------------------- 
2016-12-10 11:04:11 0.015625 
2016-12-10 11:04:11 Epoch 23 
2016-12-10 11:16:15 Training Error = 0.87338770075726 
2016-12-10 11:16:15 Training Loss = 0.014715379824822 
2016-12-10 11:16:26 Valid Error = 0.89853987270685 
2016-12-10 11:16:26 Valid Loss = 0.015345254006823 
2016-12-10 11:16:39 Test Error = 0.91108117211182 
2016-12-10 11:16:39 Test Loss = 0.01553135459066 
2016-12-10 11:16:39 -------------------LR------------------- 
2016-12-10 11:16:39 0.015625 
2016-12-10 11:16:39 Epoch 24 
2016-12-10 11:28:25 Training Error = 0.87384538570359 
2016-12-10 11:28:25 Training Loss = 0.014714236000587 
2016-12-10 11:28:36 Valid Error = 0.89853987270685 
2016-12-10 11:28:36 Valid Loss = 0.015211357478223 
2016-12-10 11:28:48 Test Error = 0.91108117211182 
2016-12-10 11:28:48 Test Loss = 0.015396809777271 
2016-12-10 11:28:48 -------------------LR------------------- 
2016-12-10 11:28:48 0.015625 
2016-12-10 11:28:48 Epoch 25 
