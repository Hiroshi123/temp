

require 'nn'
require 'cutorch'


-- implementation of squeezenet proposed in: http://arxiv.org/abs/1602.07360

local function fire(ch, s1, e1, e3)
        local net = nn.Sequential()
        net:add(nn.SpatialConvolution(ch, s1, 1, 1))
        net:add(nn.ReLU(true))
        local exp = nn.Concat(1)
        exp:add(nn.SpatialConvolution(s1, e1, 1, 1))
        exp:add(nn.SpatialConvolution(s1, e3, 3, 3, 1, 1, 1, 1))
        --exp:add(nn.SpatialConvolution(s1, e3, 3, 3))
	net:add(exp)
        net:add(nn.ReLU(true))
        return net
end


local function bypass(net)
        local cat = nn.ConcatTable()
        cat:add(net)
        cat:add(nn.Identity())
        local seq = nn.Sequential()
        seq:add(cat)
        seq:add(nn.CAddTable(true))
        return seq
end

local function squeezenet(output_classes)
        local net = nn.Sequential()
        net:add(nn.SpatialConvolution(3, 96, 7, 7, 2, 2, 0, 0)) -- conv1
	net:add(nn.ReLU(true))
        net:add(nn.SpatialMaxPooling(3, 3, 2, 2))
        net:add(fire(96, 16, 64, 64))  --fire2
        net:add(bypass(fire(128, 16, 64, 64)))  --fire3
        net:add(fire(128, 32, 128, 128))  --fire4
        net:add(nn.SpatialMaxPooling(3, 3, 2, 2))
        net:add(bypass(fire(256, 32, 128, 128)))  --fire5
        net:add(fire(256, 48, 192, 192))  --fire6
        net:add(bypass(fire(384, 48, 192, 192)))  --fire7
        net:add(fire(384, 64, 256, 256))  --fire8
        net:add(nn.SpatialMaxPooling(3, 3, 2, 2))
        net:add(bypass(fire(512, 64, 256, 256)))  --fire9
        net:add(nn.Dropout())
        net:add(nn.SpatialConvolution(512, output_classes, 1, 1, 1, 1, 1, 1)) --conv10
        net:add(nn.ReLU(true))
        net:add(nn.SpatialAveragePooling(14, 14, 1, 1))
        net:add(nn.View(output_classes))
        net:add(nn.LogSoftMax())
        return net
end

nClasses = 10

function createModel(nGPU)
   
   local model = squeezenet(nClasses)    
   print(model)   
   
   -- model:cuda()
   -- model = makeDataParallel(model, nGPU)
   -- model.imageSize = 256
   -- model.imageCrop = 224
   return model
   
end

output_classes = 10

function createSimplerModel()
   local net = nn.Sequential()
   net:add(nn.SpatialConvolution(1, 3, 1, 1))
   
   --net:add(fire(3, 4, 10, 10))

   net:add(fire(3, 5, 12, 12))
   net:add(fire(24, 4, 10, 10))
   net:add(fire(20, 4, 10, 10))
   net:add(nn.SpatialMaxPooling(28,28))
   --net:add(nn.View(20))
   net:add(nn.LogSoftMax())

   --net:add(bypass(fire(24, 4, 8, 8)))
   --net:add(nn.SpatialConvolution(16,10,3,3))
   
   --net:add(nn.LogSoftMax())
   
   return net
end


--model = createModel(1)
model = createSimplerModel()
criterion = nn.ClassNLLCriterion()

--local f = criterion:forward(outputs, targets)
--model:zeroGradParameters()

print(model)

inputs = torch.rand(100,1,28,28)

--print(inputs)

output = model:forward(inputs[1])

targets = torch.Tensor(20):fill(0)

targets[2] = 1

--criterion = nn.ClassNLLCriterion()

criterion = nn.MSECriterion() -- Mean Squared Error criterion

--trainer = nn.StochasticGradient(model, criterion)
--trainer:train(inputs)

params, gradParams = model:getParameters()

print(params:size())
print(gradParams:size())

learningRate = 0.1

function train()
   
   local f = criterion:forward(output, targets)
   model:zeroGradParameters()
   local gradCriterion = criterion:backward(output, targets)
   --print(gradCriterion)
   model:backward(inputs[1], gradCriterion)
   --print(gradParams[10])
   print(params[10])
   params:add(-learningRate, gradParams)

   --model:updateParameters(learningRate)
   
end

for i=1,10 do
   train()
end

--local df_do = criterion:backward(output, targets)

print(output:size())

--print(df_do)

--model:backward(inputs, df_do)



